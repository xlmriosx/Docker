{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN8yG8UnQAoh3SCkEBcLvTm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xlmriosx/data-projects/blob/main/how-working-install-Hadoop-with-Notebooks/how_working_install_Hadoop_with_Notebooks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üêòüìù Basic commands to work with Hadoop in Notebooks \n",
        "---\n",
        "## üîóRelated content \n",
        "### You can find post related in: \n",
        "üë®‚Äçüíª[DEV](https://dev.to/xlmriosx/how-workinginstall-hadoop-with-notebooks-34lj) \n",
        "\n",
        "### You can find video related in:\n",
        "üì∫[YouTube](https://youtu.be/tiF05Nh-zn8) \n",
        "\n",
        "### You can find repo related in:\n",
        "üê±‚Äçüèç[GitHub](https://github.com/xlmriosx/How-working-install-Hadoop-with-Notebooks) \n",
        "\n",
        "### You can connect with me in:\n",
        "üß¨[LinkedIn](https://www.linkedin.com/in/xlmriosx/) \n",
        "\n",
        "--- \n"
      ],
      "metadata": {
        "id": "FTsEc81XPR5I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Resume üßæ\n",
        "\n",
        "I will install Hadoop program and will use a library of Python to write a job that answer the question, how many row exists by each rating?"
      ],
      "metadata": {
        "id": "CGMj2ecitOH6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 1st - Install Hadoop üêò\n",
        "\n",
        "I use following command but you can change to get current last version:\n",
        "\n",
        "`!wget https://downloads.apache.org/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz`"
      ],
      "metadata": {
        "id": "NHaLUOyzXFMf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UeK7XRzd_ifg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5330b442-7b62-4f86-812d-2e38fc8a99f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-03 21:56:46--  https://downloads.apache.org/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz\n",
            "Resolving downloads.apache.org (downloads.apache.org)... 135.181.214.104, 88.99.95.219, 2a01:4f8:10a:201a::2, ...\n",
            "Connecting to downloads.apache.org (downloads.apache.org)|135.181.214.104|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 695457782 (663M) [application/x-gzip]\n",
            "Saving to: ‚Äòhadoop-3.3.4.tar.gz‚Äô\n",
            "\n",
            "hadoop-3.3.4.tar.gz 100%[===================>] 663.24M  18.3MB/s    in 38s     \n",
            "\n",
            "2023-01-03 21:57:24 (17.7 MB/s) - ‚Äòhadoop-3.3.4.tar.gz‚Äô saved [695457782/695457782]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://downloads.apache.org/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You would can get other version if you need in: https://downloads.apache.org/hadoop/common/ and later replace it in the before command.\n"
      ],
      "metadata": {
        "id": "XZzqhSEUXTPZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 2nd - Unzip and copy üîì\n",
        "\n",
        "I use following command:\n",
        "\n",
        "`!tar -xzvf hadoop-3.3.4.tar.gz && cp -r hadoop-3.3.4/ /usr/local/`"
      ],
      "metadata": {
        "id": "Op2wZe6yXkyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzvf hadoop-3.3.4.tar.gz && cp -r hadoop-3.3.4/ /usr/local/"
      ],
      "metadata": {
        "id": "WEm-Duf3juen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 3rd - Set up Hadoop's Java ‚òï\n",
        "\n",
        "I use following command:\n",
        "```\n",
        "#To find the default Java path and add export in hadoop-env.sh\n",
        "JAVA_HOME = !readlink -f /usr/bin/java | sed \"s:bin/java::\"\n",
        "java_home_text = JAVA_HOME[0]\n",
        "java_home_text_command = f\"$ {JAVA_HOME[0]} \"\n",
        "!echo export JAVA_HOME=$java_home_text >>/usr/local/hadoop-3.3.4/etc/hadoop/hadoop-env.sh\n",
        "```"
      ],
      "metadata": {
        "id": "gsWYtGi-X6UC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#To find the default Java path and add export in hadoop-env.sh\n",
        "JAVA_HOME = !readlink -f /usr/bin/java | sed \"s:bin/java::\"\n",
        "java_home_text = JAVA_HOME[0]\n",
        "java_home_text_command = f\"$ {JAVA_HOME[0]} \"\n",
        "!echo export JAVA_HOME=$java_home_text >>/usr/local/hadoop-3.3.4/etc/hadoop/hadoop-env.sh"
      ],
      "metadata": {
        "id": "vTVvI-oRkCUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 4th - Set Hadoop home variables üè°\n",
        "\n",
        "I use following command:\n",
        "```\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ['HADOOP_HOME']=\"/usr/local/hadoop-3.3.4\"\n",
        "os.environ['JAVA_HOME']=java_home_text\n",
        "```"
      ],
      "metadata": {
        "id": "JvjM0JKAZKRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['HADOOP_HOME']=\"/usr/local/hadoop-3.3.4\"\n",
        "os.environ['JAVA_HOME']=java_home_text"
      ],
      "metadata": {
        "id": "Shh47AdJkG4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 5th - Run Hadoop üèÉ‚Äç‚ôÇÔ∏è\n",
        "\n",
        "I use following command:\n",
        "\n",
        "`!/usr/local/hadoop-3.3.4/bin/hadoop`"
      ],
      "metadata": {
        "id": "mAgWqtwWZOf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.3.4/bin/hadoop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBvDH-s_kJGo",
        "outputId": "c5b63068-6a25-4e4e-9fdb-52b64d16403c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: hadoop [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]\n",
            " or    hadoop [OPTIONS] CLASSNAME [CLASSNAME OPTIONS]\n",
            "  where CLASSNAME is a user-provided Java class\n",
            "\n",
            "  OPTIONS is none or any of:\n",
            "\n",
            "buildpaths                       attempt to add class files from build tree\n",
            "--config dir                     Hadoop config directory\n",
            "--debug                          turn on shell script debug mode\n",
            "--help                           usage information\n",
            "hostnames list[,of,host,names]   hosts to use in slave mode\n",
            "hosts filename                   list of hosts to use in slave mode\n",
            "loglevel level                   set the log4j level for this command\n",
            "workers                          turn on worker mode\n",
            "\n",
            "  SUBCOMMAND is one of:\n",
            "\n",
            "\n",
            "    Admin Commands:\n",
            "\n",
            "daemonlog     get/set the log level for each daemon\n",
            "\n",
            "    Client Commands:\n",
            "\n",
            "archive       create a Hadoop archive\n",
            "checknative   check native Hadoop and compression libraries availability\n",
            "classpath     prints the class path needed to get the Hadoop jar and the\n",
            "              required libraries\n",
            "conftest      validate configuration XML files\n",
            "credential    interact with credential providers\n",
            "distch        distributed metadata changer\n",
            "distcp        copy file or directories recursively\n",
            "dtutil        operations related to delegation tokens\n",
            "envvars       display computed Hadoop environment variables\n",
            "fs            run a generic filesystem user client\n",
            "gridmix       submit a mix of synthetic job, modeling a profiled from\n",
            "              production load\n",
            "jar <jar>     run a jar file. NOTE: please use \"yarn jar\" to launch YARN\n",
            "              applications, not this command.\n",
            "jnipath       prints the java.library.path\n",
            "kdiag         Diagnose Kerberos Problems\n",
            "kerbname      show auth_to_local principal conversion\n",
            "key           manage keys via the KeyProvider\n",
            "rumenfolder   scale a rumen input trace\n",
            "rumentrace    convert logs into a rumen trace\n",
            "s3guard       manage metadata on S3\n",
            "trace         view and modify Hadoop tracing settings\n",
            "version       print the version\n",
            "\n",
            "    Daemon Commands:\n",
            "\n",
            "kms           run KMS, the Key Management Server\n",
            "registrydns   run the registry DNS server\n",
            "\n",
            "SUBCOMMAND may print help when invoked w/o parameters or with -h.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 6th - Create a folder with HDFS üåéüìÇ\n",
        "\n",
        "I use following command:\n",
        "\n",
        "`!/usr/local/hadoop-3.3.4/bin/hadoop fs -mkdir ml-100k`\n"
      ],
      "metadata": {
        "id": "5izNMk5xZeD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.3.4/bin/hadoop fs -mkdir ml-100k"
      ],
      "metadata": {
        "id": "xK4NqL-ukMBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "#### 7th - Remove folder with HDFS ‚ôª\n",
        "\n",
        "Maybe, later you need remove it. To do that you must apply following command:\n",
        "\n",
        "`!/usr/local/hadoop-3.3.4/bin/hadoop fs -rm -r ml-100k`"
      ],
      "metadata": {
        "id": "qs21OIpOdK9o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 8th - Getting a dataset to anlyze with Hadoop üíæ\n",
        "\n",
        "I use a dataset from grouplens. You can get other in:\n",
        "http://files.grouplens.org/datasets/\n",
        "\n",
        "This time I use movieslens and you can download it using:\n",
        "\n",
        "`!wget http://files.grouplens.org/datasets/movielens/ml-100k.zip`"
      ],
      "metadata": {
        "id": "S-kUs0yGdQGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://files.grouplens.org/datasets/movielens/ml-100k.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DZa2EeHkTLY",
        "outputId": "4003f784-351c-4be6-c297-5bc4bfabf6a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-03 21:58:31--  http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
            "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
            "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4924029 (4.7M) [application/zip]\n",
            "Saving to: ‚Äòml-100k.zip‚Äô\n",
            "\n",
            "ml-100k.zip         100%[===================>]   4.70M  9.90MB/s    in 0.5s    \n",
            "\n",
            "2023-01-03 21:58:32 (9.90 MB/s) - ‚Äòml-100k.zip‚Äô saved [4924029/4924029]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To use data extract files. I extract files in path later of -d in command:\n",
        "\n",
        "`!unzip \"/content/ml-100k.zip\" -d \"/content/ml-100k_folder\"`\n"
      ],
      "metadata": {
        "id": "2cHCitoVdjDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/ml-100k.zip\" -d \"/content/ml-100k_folder\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTTLRTBO5c85",
        "outputId": "9daf65e4-72b2-4fd7-a10c-f553d2a0a8fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/ml-100k.zip\n",
            "   creating: /content/ml-100k_folder/ml-100k/\n",
            "  inflating: /content/ml-100k_folder/ml-100k/allbut.pl  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/mku.sh  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/README  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u.data  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u.genre  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u.info  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u.item  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u.occupation  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u.user  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u1.base  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u1.test  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u2.base  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u2.test  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u3.base  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u3.test  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u4.base  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u4.test  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u5.base  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u5.test  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/ua.base  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/ua.test  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/ub.base  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/ub.test  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can move it in actual directory using HDFS like:\n",
        "\n",
        "`!/usr/local/hadoop-3.3.4/bin/hadoop fs -copyFromLocal /content/ml-100k_folder/ml-100k/* ml-100k/`"
      ],
      "metadata": {
        "id": "MNQzQ4ImiiV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.3.4/bin/hadoop fs -copyFromLocal /content/ml-100k_folder/ml-100k/* ml-100k/"
      ],
      "metadata": {
        "id": "q_x_6l1o5ijg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For list them:\n",
        "\n",
        "`!/usr/local/hadoop-3.3.4/bin/hadoop fs -ls ml-100k`"
      ],
      "metadata": {
        "id": "RebggKaPjlpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.3.4/bin/hadoop fs -ls ml-100k"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1azCD7x58MQ",
        "outputId": "e5c3f2d1-3bbc-41ff-e0f4-ef53c206394b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 23 items\n",
            "-rw-r--r--   1 root root       6750 2023-01-03 21:58 ml-100k/README\n",
            "-rw-r--r--   1 root root        716 2023-01-03 21:58 ml-100k/allbut.pl\n",
            "-rw-r--r--   1 root root        643 2023-01-03 21:58 ml-100k/mku.sh\n",
            "-rw-r--r--   1 root root    1979173 2023-01-03 21:58 ml-100k/u.data\n",
            "-rw-r--r--   1 root root        202 2023-01-03 21:58 ml-100k/u.genre\n",
            "-rw-r--r--   1 root root         36 2023-01-03 21:58 ml-100k/u.info\n",
            "-rw-r--r--   1 root root     236344 2023-01-03 21:58 ml-100k/u.item\n",
            "-rw-r--r--   1 root root        193 2023-01-03 21:58 ml-100k/u.occupation\n",
            "-rw-r--r--   1 root root      22628 2023-01-03 21:58 ml-100k/u.user\n",
            "-rw-r--r--   1 root root    1586544 2023-01-03 21:58 ml-100k/u1.base\n",
            "-rw-r--r--   1 root root     392629 2023-01-03 21:58 ml-100k/u1.test\n",
            "-rw-r--r--   1 root root    1583948 2023-01-03 21:58 ml-100k/u2.base\n",
            "-rw-r--r--   1 root root     395225 2023-01-03 21:58 ml-100k/u2.test\n",
            "-rw-r--r--   1 root root    1582546 2023-01-03 21:58 ml-100k/u3.base\n",
            "-rw-r--r--   1 root root     396627 2023-01-03 21:58 ml-100k/u3.test\n",
            "-rw-r--r--   1 root root    1581878 2023-01-03 21:58 ml-100k/u4.base\n",
            "-rw-r--r--   1 root root     397295 2023-01-03 21:58 ml-100k/u4.test\n",
            "-rw-r--r--   1 root root    1581776 2023-01-03 21:58 ml-100k/u5.base\n",
            "-rw-r--r--   1 root root     397397 2023-01-03 21:58 ml-100k/u5.test\n",
            "-rw-r--r--   1 root root    1792501 2023-01-03 21:58 ml-100k/ua.base\n",
            "-rw-r--r--   1 root root     186672 2023-01-03 21:58 ml-100k/ua.test\n",
            "-rw-r--r--   1 root root    1792476 2023-01-03 21:58 ml-100k/ub.base\n",
            "-rw-r--r--   1 root root     186697 2023-01-03 21:58 ml-100k/ub.test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 9th - Installing dependencies to use Python üìö\n",
        "\n",
        "We can install dependency to use MapReduce using:\n",
        "\n",
        "`!pip install mrjob`\n",
        "\n",
        "I recomend learn more about mrjob in: https://mrjob.readthedocs.io/en/latest/"
      ],
      "metadata": {
        "id": "jABHZjDfb7cI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mrjob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InLmyHPg6Igw",
        "outputId": "0749c1dc-8a3c-44e3-a9a8-743a8a3ee21a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mrjob\n",
            "  Downloading mrjob-0.7.4-py2.py3-none-any.whl (439 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m439.6/439.6 KB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.8/dist-packages (from mrjob) (6.0)\n",
            "Installing collected packages: mrjob\n",
            "Successfully installed mrjob-0.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 10th - Mananging temp folder üíø\n",
        "\n",
        "You can create in anywhere you want."
      ],
      "metadata": {
        "id": "705RlQxfdZ2-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a temp folder to use when we run job:\n",
        "\n",
        "`!/usr/local/hadoop-3.3.4/bin/hadoop fs -mkdir file:///tmp`"
      ],
      "metadata": {
        "id": "6c7e9zgKcbwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.3.4/bin/hadoop fs -mkdir file:///tmp\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GPYExIA6Lod",
        "outputId": "c012872b-0ccd-4dc1-bfc0-d78f7d7f013c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: `file:///tmp': File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We assign permissions to temp folder with:\n",
        "\n",
        "`!/usr/local/hadoop-3.3.4/bin/hadoop fs -mkdir file:///tmp`"
      ],
      "metadata": {
        "id": "johezMFac5Zp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.3.4/bin/hadoop fs -chmod 777 file:///tmp"
      ],
      "metadata": {
        "id": "VeHNlGJSMPA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We list files of temp folder:\n",
        "\n",
        "`!/usr/local/hadoop-3.3.4/bin/hadoop fs -ls file:///tmp`"
      ],
      "metadata": {
        "id": "azXyKyVYdL4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.3.4/bin/hadoop fs -ls file:///tmp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MH8qR8YVMQx4",
        "outputId": "2d9626e4-0266-477f-b960-8c03e7c5f00b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 14 items\n",
            "-rw-r--r--   1 root root       1184 2023-01-03 21:56 file:///tmp/dap_multiplexer.0256b05babb9.root.log.INFO.20230103-215632.82\n",
            "-rw-r--r--   1 root root       1184 2023-01-03 21:56 file:///tmp/dap_multiplexer.INFO\n",
            "-rwxr-xr-x   1 root root          0 2023-01-03 21:56 file:///tmp/debugger_1z7cufpo78\n",
            "drwxr-xr-x   - root root       4096 2023-01-03 22:00 file:///tmp/hsperfdata_root\n",
            "drwx------   - root root       4096 2023-01-03 21:56 file:///tmp/initgoogle_syslog_dir.0\n",
            "-rw-r--r--   1 root root      25020 2023-01-03 21:58 file:///tmp/kernel_manager_proxy.0256b05babb9.root.log.INFO.20230103-215629.34\n",
            "-rw-r--r--   1 root root        483 2023-01-03 21:56 file:///tmp/kernel_manager_proxy.0256b05babb9.root.log.WARNING.20230103-215633.34\n",
            "-rw-r--r--   1 root root      25020 2023-01-03 21:58 file:///tmp/kernel_manager_proxy.INFO\n",
            "-rw-r--r--   1 root root        483 2023-01-03 21:56 file:///tmp/kernel_manager_proxy.WARNING\n",
            "drwx------   - root root       4096 2023-01-03 21:59 file:///tmp/pyright-1078-Ls6QvpkBnsab\n",
            "drwx------   - root root       4096 2023-01-03 21:59 file:///tmp/pyright-1078-ZgRf8lO8S5wW\n",
            "drwx------   - root root       4096 2023-01-03 21:56 file:///tmp/pyright-177-8LZHFlkYsOPR\n",
            "drwx------   - root root       4096 2023-01-03 21:56 file:///tmp/pyright-177-bCSkd9NR51q0\n",
            "drwxr-xr-x   - root root       4096 2023-01-03 21:58 file:///tmp/python-languageserver-cancellation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 11th - Creating process to use with MRJOB using Python üêç\n",
        "\n",
        "To create job in Python, you must see structure of dataset to configure jobs.\n",
        "In this case dataset is like:\n",
        "\n",
        "`!head /content/ml-100k/u.data -n -10`"
      ],
      "metadata": {
        "id": "mJylNhSWe4lj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head /content/ml-100k/u.data -n 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCiSlenci44k",
        "outputId": "5b1543c1-6f8c-4c75-8257-c323bfe0bd61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "196\t242\t3\t881250949\n",
            "186\t302\t3\t891717742\n",
            "22\t377\t1\t878887116\n",
            "244\t51\t2\t880606923\n",
            "166\t346\t1\t886397596\n",
            "298\t474\t4\t884182806\n",
            "115\t265\t2\t881171488\n",
            "253\t465\t5\t891628467\n",
            "305\t451\t3\t886324817\n",
            "6\t86\t3\t883603013\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I can get following information of dataset:\n",
        "\n",
        "- First column reference to userID.\n",
        "- Second column reference to movieID.\n",
        "- Third column reference to rating.\n",
        "- Fourth column reference to timestamp."
      ],
      "metadata": {
        "id": "e-4NrtKzjkXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile RatingBreakdown.py\n",
        "# import modules\n",
        "from mrjob.job import MRJob\n",
        "from mrjob.step import MRStep\n",
        "\n",
        "# create class inhereted from MRJob\n",
        "class RatingBreakdown(MRJob):\n",
        "  # assign steps, first mapper last reducer\n",
        "  def steps(self):\n",
        "    return [\n",
        "            MRStep(mapper=self.mapper_get_rating,\n",
        "                   reducer=self.reducer_count_ratings)\n",
        "    ]\n",
        "  \n",
        "  # creating mapper, assigning attributes from dataset\n",
        "  def mapper_get_rating(self, _, line):\n",
        "    (userID, movieID, rating, timestamp) = line.split('\\t')\n",
        "    # assign like the key rating and assign each row value 1\n",
        "    yield rating, 2\n",
        "  \n",
        "  # creating reducer, sum \n",
        "  def reducer_count_ratings(self, key, values):\n",
        "    # in function of each key we sum values\n",
        "    yield key, sum(values)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  RatingBreakdown.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzIoXfzo6UnT",
        "outputId": "89914198-1582-466f-c43a-e7670f123acd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting RatingBreakdown.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 12th - Running the process üôà\n",
        "\n",
        "Here we run the process specifing some parameters:\n",
        "- Python file program `!python RatingBreakdown.py`\n",
        "- Where is .jar to run hadoop `/usr/local/hadoop-3.3.4/share/hadoop/tools/lib/hadoop-streaming-3.3.4.jar`\n",
        "- Temp file and dataset `file:///tmp /content/ml-100k/u.data`\n",
        "\n",
        "When run process, maybe take a few minutes...\n",
        "I run with:\n",
        "\n",
        "`!python RatingBreakdown.py -r hadoop --hadoop-streaming-jar /usr/local/hadoop-3.3.4/share/hadoop/tools/lib/hadoop-streaming-3.3.4.jar --hadoop-tmp-dir file:///tmp /content/ml-100k/u.data`"
      ],
      "metadata": {
        "id": "GoVLh7mxlvsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python RatingBreakdown.py -r hadoop --hadoop-streaming-jar /usr/local/hadoop-3.3.4/share/hadoop/tools/lib/hadoop-streaming-3.3.4.jar --hadoop-tmp-dir file:///tmp /content/ml-100k/u.data"
      ],
      "metadata": {
        "id": "tz-cnvfA6u2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 13th - Listing results ü•Ç\n",
        "\n",
        "I run again process and put results in results.txt\n",
        "\n",
        "`!python RatingBreakdown.py /content/ml-100k_folder/ml-100k/u.data >> results.txt`"
      ],
      "metadata": {
        "id": "VoviTaHRnYLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python RatingBreakdown.py /content/ml-100k_folder/ml-100k/u.data > results.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7Ayubm-Ca4X",
        "outputId": "29978103-aced-4482-95ed-ea0bc08ddb08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No configs found; falling back on auto-configuration\n",
            "No configs specified for inline runner\n",
            "Creating temp directory /tmp/RatingBreakdown.root.20230103.221631.300635\n",
            "Running step 1 of 1...\n",
            "job output is in /tmp/RatingBreakdown.root.20230103.221631.300635/output\n",
            "Streaming final output from /tmp/RatingBreakdown.root.20230103.221631.300635/output...\n",
            "Removing temp directory /tmp/RatingBreakdown.root.20230103.221631.300635...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use following command to list file: \n",
        "\n",
        "`!cat results.txt`"
      ],
      "metadata": {
        "id": "NIN2_tC8rnGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat results.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swxVQ9f5Cgw1",
        "outputId": "acb8fc05-7b1e-40d9-adf2-e7bc40db1582"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"1\"\t12220\n",
            "\"2\"\t22740\n",
            "\"3\"\t54290\n",
            "\"5\"\t42402\n",
            "\"4\"\t68348\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 14th - Say thanks, give like and share if this has been of help/interest üòÅüññ\n",
        "---\n"
      ],
      "metadata": {
        "id": "LupX9XxCn2Op"
      }
    }
  ]
}
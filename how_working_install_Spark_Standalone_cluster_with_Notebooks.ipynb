{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNQ6OFstu3O4e1RDp+trGVp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xlmriosx/data-projects/blob/main/how_working_install_Spark_Standalone_cluster_with_Notebooks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üåüüìù Basic commands to work with Spark in Notebooks like a Standalone cluster\n",
        "\n",
        "---\n",
        "\n",
        "## üîóRelated content \n",
        "### You can find post related in: \n",
        "üë®‚Äçüíª[DEV](https://) \n",
        "\n",
        "### You can find video related in:\n",
        "üì∫[YouTube](https://youtu.be/KxNzlGv1cKQ) \n",
        "\n",
        "### You can find repo related in:\n",
        "üê±‚Äçüèç[GitHub](https://github.com/xlmriosx/How-working-install-Spark-with-Notebooks) \n",
        "\n",
        "### You can connect with me in:\n",
        "üß¨[LinkedIn](https://www.linkedin.com/in/xlmriosx/) \n",
        "\n",
        "--- \n"
      ],
      "metadata": {
        "id": "FTsEc81XPR5I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Resume üßæ\n",
        "\n",
        "I will install Spark program and will use a library of Python to write a job that answer the question, how many row exists by each rating?\n",
        "\n",
        "Before start we setup environment to run Spark Standalone Cluster."
      ],
      "metadata": {
        "id": "CGMj2ecitOH6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 1st - Mount Google Drive üö†\n",
        "\n",
        "We will mount Google Drive to can use it files.\n",
        "\n",
        "\n",
        "I use following script:\n",
        "\n",
        "```\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "```"
      ],
      "metadata": {
        "id": "NHaLUOyzXFMf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UeK7XRzd_ifg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c40115a-179c-43ac-da5c-d1c36d004387"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 2nd - Install Spark üéá\n",
        "\n",
        "Later got a Colab notebook up, to get Spark running you have to run the following script (I apologize for how ugly it is).\n",
        "\n",
        "I use following script:\n",
        "\n",
        "```\n",
        "%%bash\n",
        "apt-get install openjdk-8-jdk-headless -qq > /dev/null \n",
        "if [[ ! -d spark-3.3.1-bin-hadoop3.tgz ]]; then\n",
        "  echo \"Spark hasn't been installed, Downloading and installing!\"\n",
        "  wget -q https://downloads.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz\n",
        "  tar xf spark-3.3.1-bin-hadoop3.tgz\n",
        "  rm -f spark-3.3.1-bin-hadoop3.tgz\n",
        "\n",
        "fi\n",
        "pip install -q findspark\n",
        "```"
      ],
      "metadata": {
        "id": "Op2wZe6yXkyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "if [[ ! -d spark-3.3.1-bin-hadoop3.tgz ]]; then\n",
        "  echo \"Spark hasn't been installed. So was downloaded and installed!\"\n",
        "  wget -q https://downloads.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz\n",
        "  tar xf spark-3.3.1-bin-hadoop3.tgz\n",
        "  rm -f spark-3.3.1-bin-hadoop3.tgz\n",
        "\n",
        "fi\n",
        "pip install -q findspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcm8sP2J3uQE",
        "outputId": "873d8963-cbff-4be6-a618-40062c4f0378"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark hasn't been installed. So was downloaded and installed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You would can get other version if you need in: https://downloads.apache.org/spark/ and later replace it in the before command.\n"
      ],
      "metadata": {
        "id": "XZzqhSEUXTPZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 3rd - Setting environment variables üåê\n",
        "\n",
        "I use following command:\n",
        "```\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"  \n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.1-bin-hadoop3\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "```"
      ],
      "metadata": {
        "id": "gsWYtGi-X6UC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"  \n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.1-bin-hadoop3\"\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "vTVvI-oRkCUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 4th - Configuring a SparkSession üö™\n",
        "\n",
        "I use following command:\n",
        "```\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\ # set up like master using all(*) threads\n",
        "    .appName(\"BLOG_XLMRIOSX\") \\ # a generic name\n",
        "    .getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "sc\n",
        "```\n",
        "\n",
        "Later of execute this we can use many data strcuture to manage data, like RDD and Dataframe(Spark and Pandas).\n",
        "Exists one more but only exists in Scala and it is Dataset."
      ],
      "metadata": {
        "id": "JvjM0JKAZKRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"BLOG_XLMRIOSX\") \\\n",
        "    .getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "sc"
      ],
      "metadata": {
        "id": "Shh47AdJkG4r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "5eb4141c-39c0-4f52-bf98-e3af5c4be519"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SparkContext master=local[*] appName=BLOG_XLMRIOSX>"
            ],
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://ee856d9f014a:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>BLOG_XLMRIOSX</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 5th - Getting a dataset to anlyze with Spark üíæ\n",
        "\n",
        "I use a dataset from grouplens. You can get other in:\n",
        "http://files.grouplens.org/datasets/\n",
        "\n",
        "This time I use movieslens and you can download it using:\n",
        "\n",
        "`!wget http://files.grouplens.org/datasets/movielens/ml-100k.zip`\n"
      ],
      "metadata": {
        "id": "5izNMk5xZeD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://files.grouplens.org/datasets/movielens/ml-100k.zip"
      ],
      "metadata": {
        "id": "xK4NqL-ukMBC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caa02675-0517-4334-e313-620c439b6437"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-10 22:44:39--  http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
            "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
            "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4924029 (4.7M) [application/zip]\n",
            "Saving to: ‚Äòml-100k.zip‚Äô\n",
            "\n",
            "ml-100k.zip         100%[===================>]   4.70M  8.70MB/s    in 0.5s    \n",
            "\n",
            "2023-01-10 22:44:40 (8.70 MB/s) - ‚Äòml-100k.zip‚Äô saved [4924029/4924029]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To use data extract files. I extract files in path later of -d in command:\n",
        "\n",
        "`!unzip \"/content/ml-100k.zip\" -d \"/content/ml-100k_folder\"`"
      ],
      "metadata": {
        "id": "PyGy9vXOtlOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/ml-100k.zip\" -d \"/content/ml-100k_folder\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddf9c743-25e6-4358-f5d7-214b7f0c082f",
        "id": "9XjPv9rNs_cK"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/ml-100k.zip\n",
            "   creating: /content/ml-100k_folder/ml-100k/\n",
            "  inflating: /content/ml-100k_folder/ml-100k/allbut.pl  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/mku.sh  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/README  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u.data  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u.genre  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u.info  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u.item  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u.occupation  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u.user  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u1.base  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u1.test  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u2.base  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u2.test  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u3.base  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u3.test  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u4.base  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u4.test  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u5.base  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u5.test  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/ua.base  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/ua.test  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/ub.base  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/ub.test  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head /content/ml-100k_folder/ml-100k/u.data -n 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-rU-0-1qeYp",
        "outputId": "7c160c4a-b4a9-4fa2-b68e-3c2c64e919a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "196\t242\t3\t881250949\n",
            "186\t302\t3\t891717742\n",
            "22\t377\t1\t878887116\n",
            "244\t51\t2\t880606923\n",
            "166\t346\t1\t886397596\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 6th - Configuring data to analyze üíø\n",
        "\n",
        "We create a variable called data where put path where data is like:\n",
        "\n",
        "`data = '/content/ml-100k_folder/ml-100k/u.data'`\n",
        "\n",
        "Later define a variable called df_spark where put information of data:\n",
        "\n",
        "`df_spark = spark.read.csv(data, inferSchema=True, header=True)`\n",
        "\n"
      ],
      "metadata": {
        "id": "A93Kz8LBuity"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = '/content/ml-100k_folder/ml-100k/u.data'\n",
        "df_spark = spark.read.csv(data, inferSchema=True, header=True)"
      ],
      "metadata": {
        "id": "bkrLHdvIscyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can inspect type of variable df_spark like:\n",
        "\n",
        "`print(type(df_spark))`"
      ],
      "metadata": {
        "id": "dQHKojk0w58r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(df_spark))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-XALIzevtK0",
        "outputId": "e515e2c4-253d-4096-e53a-8c9eb6f53545"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.sql.dataframe.DataFrame'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can inspect data frame of variable df_spark like:\n",
        "\n",
        "`df_spark.show()`"
      ],
      "metadata": {
        "id": "z99SY9nvxGun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_spark.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBsxg4IitcEN",
        "outputId": "54040513-beca-4fab-938b-7bd96dc32a2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------+\n",
            "|196\\t242\\t3\\t881250949|\n",
            "+----------------------+\n",
            "|  186\\t302\\t3\\t8917...|\n",
            "|  22\\t377\\t1\\t87888...|\n",
            "|  244\\t51\\t2\\t88060...|\n",
            "|  166\\t346\\t1\\t8863...|\n",
            "|  298\\t474\\t4\\t8841...|\n",
            "|  115\\t265\\t2\\t8811...|\n",
            "|  253\\t465\\t5\\t8916...|\n",
            "|  305\\t451\\t3\\t8863...|\n",
            "|   6\\t86\\t3\\t883603013|\n",
            "|  62\\t257\\t2\\t87937...|\n",
            "|  286\\t1014\\t5\\t879...|\n",
            "|  200\\t222\\t5\\t8760...|\n",
            "|  210\\t40\\t3\\t89103...|\n",
            "|  224\\t29\\t3\\t88810...|\n",
            "|  303\\t785\\t3\\t8794...|\n",
            "|  122\\t387\\t5\\t8792...|\n",
            "|  194\\t274\\t2\\t8795...|\n",
            "|  291\\t1042\\t4\\t874...|\n",
            "|  234\\t1184\\t2\\t892...|\n",
            "|  119\\t392\\t4\\t8861...|\n",
            "+----------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see format is incorrect so we will fix that where we configure format of data by following way:\n",
        "\n",
        "`df_spark = spark.read.csv(data, inferSchema=True, header=False, sep=\"\\t\")`\n",
        "\n"
      ],
      "metadata": {
        "id": "9JJvK-cCv2m7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_spark = spark.read.csv(data, inferSchema=True, header=False, sep=\"\\t\")"
      ],
      "metadata": {
        "id": "GAGNnscErQ8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_spark.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-azYn6yDsIu-",
        "outputId": "c67edc8c-51e5-4e94-c0c8-4b941df12492"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+---+---------+\n",
            "|_c0| _c1|_c2|      _c3|\n",
            "+---+----+---+---------+\n",
            "|196| 242|  3|881250949|\n",
            "|186| 302|  3|891717742|\n",
            "| 22| 377|  1|878887116|\n",
            "|244|  51|  2|880606923|\n",
            "|166| 346|  1|886397596|\n",
            "|298| 474|  4|884182806|\n",
            "|115| 265|  2|881171488|\n",
            "|253| 465|  5|891628467|\n",
            "|305| 451|  3|886324817|\n",
            "|  6|  86|  3|883603013|\n",
            "| 62| 257|  2|879372434|\n",
            "|286|1014|  5|879781125|\n",
            "|200| 222|  5|876042340|\n",
            "|210|  40|  3|891035994|\n",
            "|224|  29|  3|888104457|\n",
            "|303| 785|  3|879485318|\n",
            "|122| 387|  5|879270459|\n",
            "|194| 274|  2|879539794|\n",
            "|291|1042|  4|874834944|\n",
            "|234|1184|  2|892079237|\n",
            "+---+----+---+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 7th - Making a query üôà\n",
        "\n",
        "To make this we need know format of data, so I infer the following structure:\n",
        "\n",
        "- First column reference to userID.\n",
        "- Second column reference to movieID.\n",
        "- Third column reference to rating.\n",
        "- Fourth column reference to timestamp."
      ],
      "metadata": {
        "id": "jABHZjDfb7cI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will answer the question, how many movies by each rating exists..."
      ],
      "metadata": {
        "id": "e-4NrtKzjkXz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "#### 7.1. - Making a query with SQL syntax ‚ùîüõ¢\n",
        "\n",
        "- First, create a table with dataframe.\n",
        "\n",
        "`df_spark.createOrReplaceTempView(\"table\")`"
      ],
      "metadata": {
        "id": "rZ3oPXF60kJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_spark.createOrReplaceTempView(\"table\")"
      ],
      "metadata": {
        "id": "9stkacjC1z6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Second, can make query to answer question.\n",
        "\n",
        "`sql = spark.sql(\"SELECT _c2, COUNT(*) FROM table GROUP BY _c2\")`\n",
        "\n",
        "To see results:\n",
        "\n",
        "`sql.show()`"
      ],
      "metadata": {
        "id": "Dhsj_6PYoK_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sql = spark.sql(\"SELECT _c2, COUNT(*) FROM table GROUP BY _c2\")\n",
        "sql.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Nph8j6f1jfY",
        "outputId": "3e4d0406-ff8c-4f48-dfd2-6401b558edc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+\n",
            "|_c2|count(1)|\n",
            "+---+--------+\n",
            "|  1|    6110|\n",
            "|  3|   27145|\n",
            "|  5|   21201|\n",
            "|  4|   34174|\n",
            "|  2|   11370|\n",
            "+---+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "#### 7.2. - Making a query with Dataframe ‚ùîüìÑ\n",
        "\n",
        "- It's so easy make this query with dataframe.\n",
        "\n",
        "`dataframe.groupBy(\"_c2\").count().show()`"
      ],
      "metadata": {
        "id": "C13SiHre1Cco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe = df_spark"
      ],
      "metadata": {
        "id": "UJw8KnZP3CMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqQz68hL3QYk",
        "outputId": "3de520de-d41e-43b5-ff3c-25f14743b0a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _c0: integer (nullable = true)\n",
            " |-- _c1: integer (nullable = true)\n",
            " |-- _c2: integer (nullable = true)\n",
            " |-- _c3: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe.groupBy(\"_c2\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPlFh8JP2PGc",
        "outputId": "6ca977ed-48c1-47ad-92f6-e927dcf3dbdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+\n",
            "|_c2|count|\n",
            "+---+-----+\n",
            "|  1| 6110|\n",
            "|  3|27145|\n",
            "|  5|21201|\n",
            "|  4|34174|\n",
            "|  2|11370|\n",
            "+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "#### 7.3. - Making a query with RDD ‚ùîüßä\n",
        "\n",
        "- First, we transform dataframe to rdd type.\n",
        "\n",
        "`rdd = df_spark.rdd`"
      ],
      "metadata": {
        "id": "2cHJLm-m1C7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = df_spark.rdd"
      ],
      "metadata": {
        "id": "9VJIUSSm4N7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Second, make query with RDD functions.\n",
        "\n",
        "```\n",
        "rdd\\\n",
        ".groupBy(lambda x: x[2])\\\n",
        ".mapValues(lambda values: len(set(values)))\\\n",
        ".collect()\n",
        "```"
      ],
      "metadata": {
        "id": "NckqDHZFo07Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd\\\n",
        ".groupBy(lambda x: x[2])\\\n",
        ".mapValues(lambda values: len(set(values)))\\\n",
        ".collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrMVBOerN1_s",
        "outputId": "c22e7a8d-0539-4a77-c699-bb4de1d89c35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(3, 27145), (1, 6110), (2, 11370), (4, 34174), (5, 21201)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 8th - Say thanks, give like and share if this has been of help/interest üòÅüññ\n",
        "\n",
        "---\n",
        "\n",
        "## üîóRelated content \n",
        "### You can find post related in: \n",
        "üë®‚Äçüíª[DEV](https://) \n",
        "\n",
        "### You can find video related in:\n",
        "üì∫[YouTube](https://youtu.be/KxNzlGv1cKQ) \n",
        "\n",
        "### You can find repo related in:\n",
        "üê±‚Äçüèç[GitHub](https://github.com/xlmriosx/How-working-install-Spark-with-Notebooks) \n",
        "\n",
        "### You can connect with me in:\n",
        "üß¨[LinkedIn](https://www.linkedin.com/in/xlmriosx/) \n",
        "\n",
        "\n",
        "--- \n"
      ],
      "metadata": {
        "id": "LupX9XxCn2Op"
      }
    }
  ]
}